name: opentelemetry-demo-with-metrics
services:
  accountingservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/accountingservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-accountingservice
    container_name: accounting-service
    depends_on:
      kafka:
        condition: service_healthy
        required: true
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "20971520"
    environment:
      KAFKA_SERVICE_ADDR: kafka:9092
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: accountingservice
    image: ghcr.io/open-telemetry/demo:1.6.0-accountingservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    restart: unless-stopped
  adservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/adservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-adservice
    container_name: ad-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "314572800"
    environment:
      AD_SERVICE_PORT: "9555"
      FEATURE_FLAG_GRPC_SERVICE_ADDR: featureflagservice:50053
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_LOGS_EXPORTER: otlp
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: adservice
    image: ghcr.io/open-telemetry/demo:1.6.0-adservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 9555
        protocol: tcp
    restart: unless-stopped
  cartservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/cartservice/src/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-cartservice
    container_name: cart-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
      redis-cart:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "167772160"
    environment:
      ASPNETCORE_URLS: http://*:7070
      CART_SERVICE_PORT: "7070"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: cartservice
      REDIS_ADDR: redis-cart:6379
    image: ghcr.io/open-telemetry/demo:1.6.0-cartservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 7070
        protocol: tcp
    restart: unless-stopped
  checkoutservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/checkoutservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-checkoutservice
    container_name: checkout-service
    depends_on:
      cartservice:
        condition: service_started
        required: true
      currencyservice:
        condition: service_started
        required: true
      emailservice:
        condition: service_started
        required: true
      kafka:
        condition: service_healthy
        required: true
      otelcol:
        condition: service_started
        required: true
      paymentservice:
        condition: service_started
        required: true
      productcatalogservice:
        condition: service_started
        required: true
      shippingservice:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "20971520"
    environment:
      CART_SERVICE_ADDR: cartservice:7070
      CHECKOUT_SERVICE_PORT: "5050"
      CURRENCY_SERVICE_ADDR: currencyservice:7001
      EMAIL_SERVICE_ADDR: http://emailservice:6060
      KAFKA_SERVICE_ADDR: kafka:9092
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: checkoutservice
      PAYMENT_SERVICE_ADDR: paymentservice:50051
      PRODUCT_CATALOG_SERVICE_ADDR: productcatalogservice:3550
      SHIPPING_SERVICE_ADDR: shippingservice:50050
    image: ghcr.io/open-telemetry/demo:1.6.0-checkoutservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 5050
        protocol: tcp
    restart: unless-stopped
  currencyservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/currencyservice
      dockerfile: Dockerfile
      args:
        GRPC_VERSION: 1.46.0
        OPENTELEMETRY_VERSION: 1.5.0
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-currencyservice
    container_name: currency-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "20971520"
    environment:
      CURRENCY_SERVICE_PORT: "7001"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo,service.name=currencyservice
    image: ghcr.io/open-telemetry/demo:1.6.0-currencyservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 7001
        protocol: tcp
    restart: unless-stopped
  dataprepper:
    image: opensearchproject/data-prepper:latest
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 21892
        protocol: tcp
    volumes:
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/opensearch/pipelines.yaml
        target: /usr/share/data-prepper/pipelines/pipelines.yaml
        bind:
          create_host_path: true
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/opensearch/data-prepper-config.yaml
        target: /usr/share/data-prepper/config/data-prepper-config.yaml
        bind:
          create_host_path: true
  emailservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/emailservice
      dockerfile: Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-emailservice
    container_name: email-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "104857600"
    environment:
      APP_ENV: production
      EMAIL_SERVICE_PORT: "6060"
      OTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://otelcol:4318/v1/traces
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: emailservice
    image: ghcr.io/open-telemetry/demo:1.6.0-emailservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 6060
        protocol: tcp
    restart: unless-stopped
  featureflagservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/featureflagservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-featureflagservice
    container_name: feature-flag-service
    depends_on:
      ffs_postgres:
        condition: service_healthy
        required: true
    deploy:
      resources:
        limits:
          memory: "183500800"
    environment:
      DATABASE_URL: ecto://ffs:ffs@ffs_postgres:5432/ffs
      FEATURE_FLAG_GRPC_SERVICE_PORT: "50053"
      FEATURE_FLAG_SERVICE_PORT: "8081"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_TRACES_PROTOCOL: grpc
      OTEL_SERVICE_NAME: featureflagservice
    healthcheck:
      test:
        - CMD
        - curl
        - -H
        - 'baggage: synthetic_request=true'
        - -f
        - http://localhost:8081
    image: ghcr.io/open-telemetry/demo:1.6.0-featureflagservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8081
        protocol: tcp
      - mode: ingress
        target: 50053
        protocol: tcp
    restart: unless-stopped
  ffs_postgres:
    container_name: postgres
    deploy:
      resources:
        limits:
          memory: "125829120"
    environment:
      POSTGRES_DB: ffs
      POSTGRES_PASSWORD: ffs
      POSTGRES_USER: ffs
    healthcheck:
      test:
        - CMD-SHELL
        - pg_isready -d ffs -U ffs
      timeout: 5s
      interval: 10s
      retries: 5
    image: postgres:16.0
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    restart: unless-stopped
    user: postgres
  frauddetectionservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/frauddetectionservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-frauddetectionservice
    container_name: frauddetection-service
    depends_on:
      kafka:
        condition: service_healthy
        required: true
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "209715200"
    environment:
      KAFKA_SERVICE_ADDR: kafka:9092
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: frauddetectionservice
    image: ghcr.io/open-telemetry/demo:1.6.0-frauddetectionservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    restart: unless-stopped
  frontend:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/frontend/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-frontend
    container_name: frontend
    depends_on:
      adservice:
        condition: service_started
        required: true
      cartservice:
        condition: service_started
        required: true
      checkoutservice:
        condition: service_started
        required: true
      currencyservice:
        condition: service_started
        required: true
      otelcol:
        condition: service_started
        required: true
      productcatalogservice:
        condition: service_started
        required: true
      quoteservice:
        condition: service_started
        required: true
      recommendationservice:
        condition: service_started
        required: true
      shippingservice:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "209715200"
    environment:
      AD_SERVICE_ADDR: adservice:9555
      CART_SERVICE_ADDR: cartservice:7070
      CHECKOUT_SERVICE_ADDR: checkoutservice:5050
      CURRENCY_SERVICE_ADDR: currencyservice:7001
      ENV_PLATFORM: local
      FRONTEND_ADDR: frontend:8080
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: frontend
      PORT: "8080"
      PRODUCT_CATALOG_SERVICE_ADDR: productcatalogservice:3550
      PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://localhost:8080/otlp-http/v1/traces
      RECOMMENDATION_SERVICE_ADDR: recommendationservice:9001
      SHIPPING_SERVICE_ADDR: shippingservice:50050
      WEB_OTEL_SERVICE_NAME: frontend-web
    image: ghcr.io/open-telemetry/demo:1.6.0-frontend
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8080
        protocol: tcp
    restart: unless-stopped
  frontendproxy:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: src/frontendproxy/Dockerfile
    container_name: frontend-proxy
    depends_on:
      featureflagservice:
        condition: service_started
        required: true
      frontend:
        condition: service_started
        required: true
      grafana:
        condition: service_started
        required: true
      jaeger:
        condition: service_started
        required: true
      loadgenerator:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "52428800"
    environment:
      ENVOY_PORT: "8080"
      FEATURE_FLAG_SERVICE_HOST: feature-flag-service
      FEATURE_FLAG_SERVICE_PORT: "8081"
      FRONTEND_HOST: frontend
      FRONTEND_PORT: "8080"
      GRAFANA_SERVICE_HOST: grafana
      GRAFANA_SERVICE_PORT: "3000"
      JAEGER_SERVICE_HOST: jaeger
      JAEGER_SERVICE_PORT: "16686"
      LOCUST_WEB_HOST: loadgenerator
      LOCUST_WEB_PORT: "8089"
      OTEL_COLLECTOR_HOST: otelcol
      OTEL_COLLECTOR_PORT_GRPC: "4317"
      OTEL_COLLECTOR_PORT_HTTP: "4318"
      SEQ_FRONTEND_INTERNAL: "80"
      SEQ_HOST: seq
    image: ghcr.io/open-telemetry/demo:1.6.0-frontendproxy
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8080
        published: "8080"
        protocol: tcp
      - mode: ingress
        target: 10000
        published: "10000"
        protocol: tcp
  grafana:
    container_name: grafana
    deploy:
      resources:
        limits:
          memory: "104857600"
    environment:
      GF_INSTALL_PLUGINS: grafana-opensearch-datasource
    image: grafana/grafana:10.2.0
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 3000
        protocol: tcp
    volumes:
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/grafana/grafana.ini
        target: /etc/grafana/grafana.ini
        bind:
          create_host_path: true
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/grafana/provisioning
        target: /etc/grafana/provisioning
        bind:
          create_host_path: true
  jaeger:
    command:
      - --memory.max-traces=8000
      - --query.base-path=/jaeger/ui
      - --prometheus.server-url=http://prometheus:9090
      - --prometheus.query.normalize-calls=true
      - --prometheus.query.normalize-duration=true
    container_name: jaeger
    deploy:
      resources:
        limits:
          memory: "314572800"
    environment:
      METRICS_STORAGE_TYPE: prometheus
    image: jaegertracing/all-in-one:1.51
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 16686
        protocol: tcp
      - mode: ingress
        target: 4317
        protocol: tcp
    restart: unless-stopped
  kafka:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/kafka/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-kafka
    container_name: kafka
    deploy:
      resources:
        limits:
          memory: "524288000"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_HEAP_OPTS: -Xmx200m -Xms200m
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: kafka
    healthcheck:
      test:
        - CMD-SHELL
        - nc -z kafka 9092
      timeout: 10s
      interval: 5s
      retries: 10
      start_period: 10s
    image: ghcr.io/open-telemetry/demo:1.6.0-kafka
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    restart: unless-stopped
  loadgenerator:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/loadgenerator/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-loadgenerator
    container_name: load-generator
    depends_on:
      frontend:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "125829120"
    environment:
      LOCUST_AUTOSTART: "true"
      LOCUST_HEADLESS: "false"
      LOCUST_HOST: http://frontend-proxy:8080
      LOCUST_USERS: "10"
      LOCUST_WEB_PORT: "8089"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: loadgenerator
      PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION: python
    image: ghcr.io/open-telemetry/demo:1.6.0-loadgenerator
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8089
        protocol: tcp
    restart: unless-stopped
  opensearch:
    container_name: opensearch
    environment:
      DISABLE_INSTALL_DEMO_CONFIG: "true"
      DISABLE_SECURITY_PLUGIN: "true"
      OPENSEARCH_JAVA_OPTS: -Xms512m -Xmx512m
      bootstrap.memory_lock: "true"
      cluster.name: demo-cluster
      discovery.type: single-node
      node.name: demo-node
      script.max_compilations_rate: 1500/5m
    image: opensearchproject/opensearch:latest
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 9200
        published: "9200"
        protocol: tcp
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
  otelcol:
    command:
      - --config=/etc/otelcol-config.yml
      - --config=/etc/otelcol-config-extras.yml
    container_name: otel-col
    depends_on:
      jaeger:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "131072000"
    environment:
      ENVOY_PORT: "8080"
    image: otel/opentelemetry-collector-contrib:0.88.0
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 4317
        protocol: tcp
      - mode: ingress
        target: 4318
        protocol: tcp
    restart: unless-stopped
    volumes:
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/otelcollector/otelcol-config.yml
        target: /etc/otelcol-config.yml
        bind:
          create_host_path: true
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/otelcollector/otelcol-config-extras.yml
        target: /etc/otelcol-config-extras.yml
        bind:
          create_host_path: true
  paymentservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/paymentservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-paymentservice
    container_name: payment-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "125829120"
    environment:
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: paymentservice
      PAYMENT_SERVICE_PORT: "50051"
    image: ghcr.io/open-telemetry/demo:1.6.0-paymentservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 50051
        protocol: tcp
    restart: unless-stopped
  productcatalogservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/productcatalogservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-productcatalogservice
    container_name: product-catalog-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "20971520"
    environment:
      FEATURE_FLAG_GRPC_SERVICE_ADDR: featureflagservice:50053
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: productcatalogservice
      PRODUCT_CATALOG_SERVICE_PORT: "3550"
    image: ghcr.io/open-telemetry/demo:1.6.0-productcatalogservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 3550
        protocol: tcp
    restart: unless-stopped
  prometheus:
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
      - --enable-feature=otlp-write-receiver
    container_name: prometheus
    deploy:
      resources:
        limits:
          memory: "314572800"
    image: quay.io/prometheus/prometheus:v2.47.2
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 9090
        published: "9090"
        protocol: tcp
    volumes:
      - type: bind
        source: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics/src/prometheus/prometheus-config.yaml
        target: /etc/prometheus/prometheus-config.yaml
        bind:
          create_host_path: true
  quoteservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/quoteservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-quoteservice
    container_name: quote-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "41943040"
    environment:
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4318
      OTEL_PHP_AUTOLOAD_ENABLED: "true"
      OTEL_PHP_INTERNAL_METRICS_ENABLED: "true"
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: quoteservice
      QUOTE_SERVICE_PORT: "8090"
    image: ghcr.io/open-telemetry/demo:1.6.0-quoteservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8090
        protocol: tcp
    restart: unless-stopped
  recommendationservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/recommendationservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-recommendationservice
    container_name: recommendation-service
    depends_on:
      featureflagservice:
        condition: service_started
        required: true
      otelcol:
        condition: service_started
        required: true
      productcatalogservice:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "524288000"
    environment:
      FEATURE_FLAG_GRPC_SERVICE_ADDR: featureflagservice:50053
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4317
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      OTEL_PYTHON_LOG_CORRELATION: "true"
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: recommendationservice
      PRODUCT_CATALOG_SERVICE_ADDR: productcatalogservice:3550
      PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION: python
      RECOMMENDATION_SERVICE_PORT: "9001"
    image: ghcr.io/open-telemetry/demo:1.6.0-recommendationservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 9001
        protocol: tcp
    restart: unless-stopped
  redis-cart:
    container_name: redis-cart
    deploy:
      resources:
        limits:
          memory: "20971520"
    image: redis:7.2-alpine
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 6379
        protocol: tcp
    restart: unless-stopped
    user: redis
  seq:
    container_name: seq
    environment:
      ACCEPT_EULA: "Y"
      SEQ_FIRSTRUN_ADMINUSERNAME: admin
    image: datalust/seq:latest
    networks:
      default: null
    ports:
      - mode: ingress
        target: 80
        protocol: tcp
      - mode: ingress
        target: 5341
        published: "5341"
        protocol: tcp
    restart: always
    volumes:
      - type: volume
        source: seq_data
        target: /data
        volume: {}
  shippingservice:
    build:
      context: /Users/Mathias/Projects/Rider/opentelemetry-demo-with-metrics
      dockerfile: ./src/shippingservice/Dockerfile
      cache_from:
        - ghcr.io/open-telemetry/demo:1.6.0-shippingservice
    container_name: shipping-service
    depends_on:
      otelcol:
        condition: service_started
        required: true
    deploy:
      resources:
        limits:
          memory: "20971520"
    environment:
      OTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://otelcol:4317/v1/traces
      OTEL_RESOURCE_ATTRIBUTES: service.namespace=opentelemetry-demo
      OTEL_SERVICE_NAME: shippingservice
      QUOTE_SERVICE_ADDR: http://quoteservice:8090
      SHIPPING_SERVICE_PORT: "50050"
    image: ghcr.io/open-telemetry/demo:1.6.0-shippingservice
    logging:
      driver: json-file
      options:
        max-file: "2"
        max-size: 5m
    networks:
      default: null
    ports:
      - mode: ingress
        target: 50050
        protocol: tcp
    restart: unless-stopped
networks:
  default:
    name: opentelemetry-demo
    driver: bridge
volumes:
  seq_data:
    name: opentelemetry-demo-with-metrics_seq_data
    driver: local
x-default-logging:
  driver: json-file
  options:
    max-file: "2"
    max-size: 5m

OpenTelemetry config was shown.
